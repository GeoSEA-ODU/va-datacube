{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a961b5",
   "metadata": {},
   "source": [
    "![data cube logo](https://static.wixstatic.com/media/8959d6_a13dc7ece5be4678af57c8f7c1667734~mv2.png/v1/fill/w_279,h_177,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/VSDC_DataCubeSplash_6Partner_edited.png)\n",
    "# EPA Dasymetric Tool Implementation for Population\n",
    "\n",
    " - Sign up to the VA Datacube (INSERT LINK) to run this notebook interactively from a browser\n",
    " - <b>Compatibility</b>: Notebook currently compatible with both the NCI and DEA Sandbox environments    \n",
    "\n",
    " - <b>Products used</b>:\n",
    "    \n",
    "    products....\n",
    " \n",
    " - <b>Special requirements:</b> An optional description of any special requirements, e.g. If running on the NCI, ensure that\n",
    "    module load otps is run prior to launching this notebook\n",
    " \n",
    " - <b>Prerequisites:</b> An <i>optional</i> list of any notebooks that should be run or content that should be understood\n",
    "    prior to launching this notebook\n",
    " \n",
    " - <b>Notebook Use:</b> If you would like to modify and save this notebook, please copy and paste it into your own folder on the data cube, here is how:\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bba8c7",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "An <i>optional</i> overview of the scientific, economic or environmental management issue or challenge being addressed by Digital Earth Australia. For Beginners_Guide or Frequently_Used_Code notebooks, this may include information about why the particular technique or approach is useful or required. If you need to cite a scientific paper or link to a website, use a persistent DOI link if possible and link in-text (e.g. Dhu et al. 2017).\n",
    "\n",
    "## Overview\n",
    "Dasymetric mapping is a geospatial technique that uses information such as land cover types to more accurately distribute data within selected boundaries like census blocks.\n",
    "\n",
    "The Intelligent Dasymetric Mapping (IDM) Toolbox is available to download. This toolbox uses Python 3.6 and open source GIS libraries. An additional version is available as a [toolbox for ESRI ArcGIS Pro](https://github.com/USEPA/Dasymetric-Toolbox-ArcGISPro)\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://www.epa.gov/sites/default/files/2015-07/dasymetric_728x210.jpg\" alt=\"Dasymetric example image\"/>\n",
    "  <figcaption><sup>The image on the left shows a map of the population by block group based solely on the census data. The image on the right shows the dasymetric population allocation for several block groups in Tampa, Fla.</sup></figcaption>  \n",
    "</figure>\n",
    "<br>\n",
    "\n",
    "\n",
    "-   EnviroAtlas researchers use the dasymetric data to calculate the distribution of ecosystem services, and other metrics including walking distances, viewsheds, resource use, and exposure potential.\n",
    "-   For more information on the Dasymetric data created for EnviroAtlas, see our [website](https://www.epa.gov/enviroatlas/dasymetric-toolbox) or  [Dasymetric Allocation of Population Fact Sheet](https://enviroatlas.epa.gov/enviroatlas/DataFactSheets/pdf/Supplemental/DasymetricAllocationofPopulation.pdf).\n",
    "\n",
    "The EnviroAtlas Intelligent Dasymetric Toolbox for Open Source GIS is currently in development. The most recent release for ESRI ArcMap 10.3 is available at [https://www.epa.gov/enviroatlas/dasymetric-toolbox](https://www.epa.gov/enviroatlas/dasymetric-toolbox).\n",
    "\n",
    "\n",
    "- Name: Open source Intelligent Dasymetric Mapping (IDM) script\n",
    "\n",
    "- Author: Anam Khan\n",
    "\n",
    "- Date: 5/1/19\n",
    "\n",
    "- Description: Intelligent Dasymetric Mapping (IDM) disaggregates population \n",
    "    counts enumerated by vector source units to the spatial resolution of a \n",
    "    categorical ancillary raster containing classes that are indicative of the \n",
    "    spatial distribution of population within the source units. This script is \n",
    "    an open source version of the EnviroAtlas IDM toolbox developed by \n",
    "    Torrin Hultgren for ArcMap: https://www.epa.gov/enviroatlas/dasymetric-toolbox.\n",
    "    This version follows the publication by Mennis and Hultgren (2006) with the\n",
    "    exception that class densities for unsampled ancillary classes are calculated\n",
    "    using census polygons where the population estimated for sampled/preset \n",
    "    ancillary classes did not exceed the census population.\n",
    "\n",
    "- Modified on: 09/27/2023\n",
    "- By: Blake Steiner\n",
    "- Purpose: To create a Virginia Datacube friendly version for population vulnerability in other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3370de1",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "A <i>compulsory</i> description of the notebook, including a brief overview of how Digital Earth Australia helps to address the problem set out above. It can be good to include a run-down of the tools/methods that will be demonstrated in the notebook:\n",
    "\n",
    "1. Load the necessary libraries\n",
    "2. Call in and visualize census data\n",
    "3. Call in and visualize NLCD \n",
    "4. Modify landcover weights\n",
    "5. Run and visualize IDM function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10a543",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d817a9",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "\n",
    "Provide any particular instructions that the user might need, e.g. To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881476c",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. Begin with any iPython magic commands, followed by standard Python packages, then any additional functionality you need from the Scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c851db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def nb_dist(x, y):\n",
      "/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def get_faces(triangle):\n",
      "/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n",
      "/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def nb_mask_faces(mask, faces):\n",
      "/opt/tljh/user/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, json\n",
    "from osgeo import gdal, ogr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import argparse as ap\n",
    "\n",
    "#For HTML requests \n",
    "import json\n",
    "import requests\n",
    "import cenpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36877232",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data. The app parameter is a unique name for the analysis which is based on the notebook file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe072223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='EPA_Dasymetric')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0ded8",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "An optional section to inform the user of any parameters they'll need to configure to run the notebook:\n",
    "- popFeat_path = the path to the feature class\n",
    "- popCountField = the name of the total population (or other total count) variable in the feature class\n",
    "- popKeyField = the unique identifier for each census block or tract\n",
    "- ancRaster_path = the raster to be used to apply the dasymetric mapping technique, which is typically a land cover raster \n",
    "- out_dir = where you want the results saved to\n",
    "- API_KEY: This is <b> YOUR </b> API key you requested from the U.S. Census. \n",
    " - BASE_URL: The base url we will make all of our API calls from, which will make reference to the 2020 U.S. decennial (dec) census. \n",
    " - state: the state number, which is 51 for Virginia. \n",
    " - variables: a python dictionary of variables you want to call into the table. \n",
    "   - <b> Note: </b> there are many more than these and see the link below for a full list.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb60c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For EPA Tool\n",
    "popFeat_path = 'example_value'\n",
    "popCountField = 'example_value'\n",
    "popKeyField = 'example_value'\n",
    "ancRaster_path = 'example'\n",
    "out_dir = 'example'\n",
    "\n",
    "#Census API\n",
    "\n",
    "#Full API reference sheet: https://api.census.gov/data.html\n",
    "\n",
    "# YOUR API Key\n",
    "API_KEY = '89928aaa5288dd9ae9871dbb19a02344edc16953'\n",
    "\n",
    "#Base URL to base the requests off of\n",
    "BASE_URL = 'https://api.census.gov/data/2020/dec/dhc'\n",
    "\n",
    "#Sate and dataset type\n",
    "state = '51'  # Virginia\n",
    "\n",
    "#Variables we want to retrieve (i.e. creating a dictionary), can find more details on variables here: https://api.census.gov/data/2020/dec/pl/variables.html\n",
    "\n",
    "params = {\n",
    "    'get': 'GEO_ID, NAME, P1_001N',  # Variables to retrieve\n",
    "    'for': 'tract',              # Retrieve data at the block level\n",
    "    'in': 'state:51',              # Virginia state code\n",
    "    'key': API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Alternate way\n",
    "variables = {\n",
    "    'NAME': 'Geography Name',\n",
    "    'GEO_ID': 'GEOID',\n",
    "    'P1_001N': 'Total Population'\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d270a",
   "metadata": {},
   "source": [
    "## Census Data and API\n",
    "\n",
    "Here, we will request for census tract data across Virginia. We will check it and visualize it across the state. Finally, we will output it as a shapefile for use in other software or sharing. \n",
    "\n",
    "You can find a more in depth notebook using the US Census API here: *va-datacube/Frequently_used_code/CensusData.ipynb*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4ff41",
   "metadata": {},
   "source": [
    "### Request for Census Block Data\n",
    "\n",
    "Let's request the data and make sure our request went through. The `df` variable will be in a json form. We will need to rework it into a dataframe for visualization after the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b209aea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cenpy.products' has no attribute 'Decennial2020'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m----> 6\u001b[0m decennial_2020 \u001b[38;5;241m=\u001b[39m \u001b[43mcenpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproducts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecennial2020\u001b[49m()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Make request\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#c = \u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cenpy.products' has no attribute 'Decennial2020'"
     ]
    }
   ],
   "source": [
    "# Get census block data\n",
    "url = f'{BASE_URL}?get={\",\".join(variables.keys())}&for=tract:*&in=state:{state}&key={API_KEY}'\n",
    "response = requests.get(url)\n",
    "data = json.loads(response.text)\n",
    "\n",
    "decennial_2020 = cenpy.products\n",
    "\n",
    "#Make request\n",
    "#c = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "915d9f74-8c96-4402-beba-ac74278db91c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             NAME        GEOID  P1_001N state  \\\n",
      "0  Census Tract 4516.01; Fairfax County; Virginia  51059451601     6129    51   \n",
      "1     Census Tract 4514; Fairfax County; Virginia  51059451400     3263    51   \n",
      "2  Census Tract 4515.01; Fairfax County; Virginia  51059451501     5868    51   \n",
      "3  Census Tract 4515.02; Fairfax County; Virginia  51059451502     4924    51   \n",
      "4  Census Tract 4516.02; Fairfax County; Virginia  51059451602     2885    51   \n",
      "\n",
      "  county   tract  \n",
      "0    059  451601  \n",
      "1    059  451400  \n",
      "2    059  451501  \n",
      "3    059  451502  \n",
      "4    059  451602  \n"
     ]
    }
   ],
   "source": [
    "# Convert JSON data to a DataFrame\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# Remove first 9 characters from the 'GEO_ID' column\n",
    "df['GEO_ID'] = df['GEO_ID'].str.slice(9)\n",
    "\n",
    "# Rename the 'GEO_ID' column to 'GEOID'\n",
    "df = df.rename(columns={'GEO_ID': 'GEOID'})\n",
    "\n",
    "# Convert the 'P1_001N' column to numeric type\n",
    "df['P1_001N'] = pd.to_numeric(df['P1_001N'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b4bb5-22c2-40b9-adbd-69188593804d",
   "metadata": {},
   "source": [
    "### Plot Total Population Data\n",
    "Great! This will make it easier to do some simple plots with. Let's make a bar chart in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da5264a4-34e4-4416-a9f5-bb28afb3a8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'block'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plotting the filtered data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[43mfiltered_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP1_001N\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCensus Block\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Population\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'block'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the list of cities to filter\n",
    "cities = ['Norfolk']\n",
    "\n",
    "# Filter the DataFrame by census tracts containing the cities\n",
    "filtered_df = df[df['NAME'].str.contains('|'.join(cities))]\n",
    "\n",
    "# Plotting the filtered data\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.bar(filtered_df['block'], filtered_df['P1_001N'])\n",
    "plt.xlabel('Census Block')\n",
    "plt.ylabel('Total Population')\n",
    "plt.title('Total Population by Census Tract (Filtered)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc2c38-99f9-416d-bb1c-aa1f5122ea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9c43f-bd96-46af-9569-685bbd6daece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98914fc3",
   "metadata": {},
   "source": [
    "## Dasymetric IDM Function \n",
    "\n",
    "First, we will start by looking at the config.json and modify the weights and even language when using non NLCD datasets. After, we will go over the EPA's IDM function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ea94e3-2a84-4426-8715-239b4979c3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"11\": 0.0,\n",
      "    \"12\": 0.0,\n",
      "    \"95\": 0.0,\n",
      "    \"0\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read in and view the default config.json\n",
    "# Define the file paths\n",
    "input_file_path = \"./config.json\"  \n",
    "output_file_path = \"./norfolk_config.json\"  \n",
    "\n",
    "# Read the JSON file\n",
    "with open(input_file_path, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Print the JSON data\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3a69d-19e1-4497-9bf4-832bea9b5ca5",
   "metadata": {},
   "source": [
    "We see that classes 11, 12, 95, and 0 all have a weight of 0.0, meaning no population will be allocated to these spots. \n",
    "All other land classes are assumed to have a weight of 1.0. We will change this to have different weights and land classes in the NLCD schema being applied to the resampled 10-meter 2016 land cover data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c24b09-ecb6-45db-9348-816c2dbe909f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"2\": 0,\n",
      "    \"5\": 0,\n",
      "    \"3\": 0,\n",
      "    \"14\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Modify the text inside the JSON\n",
    "# Original JSON data\n",
    "original_data = {\n",
    "    \"11\": 0.0,\n",
    "    \"12\": 0.0,\n",
    "    \"95\": 0.0,\n",
    "    \"0\": 0.0\n",
    "}\n",
    "\n",
    "# Create a dictionary for the desired format\n",
    "new_data = {\n",
    "    \"2\": 0,\n",
    "    \"5\": 0,\n",
    "    \"3\": 0,\n",
    "    \"14\": 0\n",
    "}\n",
    "\n",
    "# Create a new dictionary by mapping the keys from the new_data to the original_data\n",
    "modified_data = {new_key: original_data.get(old_key, 0) for new_key, old_key in new_data.items()}\n",
    "\n",
    "# Print the modified data\n",
    "print(json.dumps(modified_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64cfda-e8de-4872-9357-19daf1d2f1f0",
   "metadata": {},
   "source": [
    "Below is the whole function for doing an intelligent dasymetric mapping of population. You may find more information above on their github and please attribute accordingly. \n",
    "Please look at the function carefully to understand how the parameters described earlier are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f55b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDM function from EPA\n",
    "def dasy_map (popFeat_path, popCountField, popKeyField, ancRaster_path, \n",
    "              out_dir,  popAreaMin = 1, sampleMin = 3, percent = 0.95, \n",
    "              uninhab_path = False, anc_nd = 0, pop_nd = 0):\n",
    "    '''\n",
    "    Prepare population density rasters given population and ancillary data \n",
    "    through intelligent dasymetric mapping. -popFeat_path: The path to the \\\n",
    "    census polygons with unique identifiers and a count of the population for \\\n",
    "    each polygon. - popCountField: The field in the population_features that \\\n",
    "    stores the polygon's populations.- popKeyField: The unique identifier \\\n",
    "    field for each polygon in population_features. -ancRaster_path: The path \\\n",
    "    to the land cover raster that is used for dasymetric population mapping. \\\n",
    "    -out_dir: The directory where all outputs will be saved. \\\n",
    "    -popAreaMin: The minimum number of raster cells in a source polygon for \\\n",
    "    it to be considered representative of a class (default = 1). -sampleMin: \\\n",
    "    The minimum number of source units to ensure a representative sample for \\\n",
    "    a land cover class (default = 3). -percent: The minimum percent of a \\\n",
    "    source polygon's area that an ancillary class must cover in order for the \\\n",
    "    source polygon to be considered representative of that class. Enter as a \\\n",
    "    decimal (default = 0.95). -uninhab_path: An optional shapefile containing \\\n",
    "    uninhabited areas. -anc_nd: The NoData value for the ancillary raster \\\n",
    "    (default = 0). -pop_nd: The NoData value for the population \\\n",
    "    raster (default = 0)   \n",
    "    '''\n",
    "    #Set config.json file in the script's directory to presetTable\n",
    "    if __name__ == '__main__':\n",
    "        presetTable = os.path.join(sys.path[0], \"config.json\")\n",
    "    else:\n",
    "        presetTable = os.path.join(sys.path[-1], \"config.json\")\n",
    "    \n",
    "    print ('population_features path: {0}'.format(popFeat_path))\n",
    "    print ('population_count_field: {0}'.format(popCountField))\n",
    "    print ('population_key_field: {0}'.format(popKeyField))\n",
    "    print ('ancillary_raster: {0}'.format(ancRaster_path))\n",
    "    print ('uninhabited_file: {0}'.format(uninhab_path))\n",
    "    print ('The minimum populated area of a representative unit is ' + str(popAreaMin))\n",
    "    print ('The minimum sample size is ' + str(sampleMin))\n",
    "    print ('The percent is ' + str(percent))\n",
    "    print ('The NoData value for the population raster is ' + str(pop_nd))\n",
    "    print ('The NoData value for the ancillary raster is ' + str(anc_nd))\n",
    "\n",
    "    #Set file names for outputs\n",
    "    popRaster = os.path.join(out_dir, \"PopRaster.tif\") #joins the system path to the output path specified\n",
    "    popWorkTable = os.path.join(out_dir, \"PopTable.csv\")\n",
    "    dasyRaster = os.path.join(out_dir, \"DasyRaster.tif\")\n",
    "    dasyWorkTable = os.path.join(out_dir, \"DasyWorkTable.csv\")\n",
    "    densityRaster = os.path.join(out_dir, \"DensityRaster.tif\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Set driver for raster creation and read in census population features and \\\n",
    "    ancillary raster \\\n",
    "    \"\"\"\n",
    "    rast_driver = gdal.GetDriverByName('GTiff')\n",
    "    ancRaster = gdal.Open(ancRaster_path)\n",
    "    popFeatures = ogr.Open(popFeat_path)\n",
    "    popLayer = popFeatures.GetLayer()\n",
    "    \n",
    "    '''\n",
    "    Get GeoTransform from ancillary raster: rows, columns, \\\n",
    "    coordinates of upperleft corner for north up images, and projection. \\\n",
    "    '''\n",
    "    rows = ancRaster.RasterYSize\n",
    "    cols = ancRaster.RasterXSize\n",
    "    ulx = ancRaster.GetGeoTransform()[0]\n",
    "    uly = ancRaster.GetGeoTransform()[3]\n",
    "    anc_proj = ancRaster.GetProjection()\n",
    "    \n",
    "    \"\"\"\n",
    "    Create population raster from census population features using the \\\n",
    "    GeoTransform from the ancillary raster. Set pop_nd as the NoData value \\\n",
    "    for the population raster. \\\n",
    "    \"\"\"\n",
    "    print (\"Creating population raster...\")\n",
    "    popRast = rast_driver.Create(popRaster, cols, rows, 1, \n",
    "                                gdal.GDT_Float32, options=[\"COMPRESS=LZW\"])\n",
    "    popRast.SetGeoTransform((ulx, ancRaster.GetGeoTransform()[1], 0, \n",
    "                             uly, 0, ancRaster.GetGeoTransform()[5]))\n",
    "    popRast.SetProjection(anc_proj)\n",
    "    popRast.GetRasterBand(1).SetNoDataValue(pop_nd)\n",
    "    gdal.RasterizeLayer(popRast, [1], popLayer, \n",
    "                        options = [\"ATTRIBUTE=\" + popKeyField])\n",
    "    popRast = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Burn the NoData value from the ancillary raster into the pixels that \\\n",
    "    overlap uninhabited areas \\\n",
    "    \"\"\"\n",
    "    if uninhab_path:\n",
    "        uninhab_ds = ogr.Open(uninhab_path)\n",
    "        uninhabLayer = uninhab_ds.GetLayer()\n",
    "        uninhab_anc = os.path.join(out_dir, \"uninhab_landcover.tif\")\n",
    "        uninhab_rast = rast_driver.CreateCopy(uninhab_anc, \n",
    "                                              gdal.Open(ancRaster_path), \n",
    "                                              options=['COMPRESS=LZW'])\n",
    "        gdal.RasterizeLayer(uninhab_rast, [1], uninhabLayer, \n",
    "                            burn_values = [anc_nd])\n",
    "        uninhab_rast = None\n",
    "        uninhab_ds = None\n",
    "        ancRaster = gdal.Open(uninhab_anc)\n",
    "    \n",
    "    print (\"Creating dasymetric units...\")\n",
    "    #Read ancillary raster and population raster as array\n",
    "    anc_arr = ancRaster.GetRasterBand(1).ReadAsArray().astype(np.uint64)\n",
    "    popRast = gdal.Open(popRaster)\n",
    "    pop_arr = popRast.GetRasterBand(1).ReadAsArray().astype(np.uint64)\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert pixel values of ancillary raster that do not overlap with census \\\n",
    "    polygons to NoData. \\\n",
    "    \"\"\"\n",
    "    anc_arr[pop_arr == pop_nd] = anc_nd\n",
    "    \n",
    "    \"\"\"\n",
    "    Combine the ancillary raster and the population raster using Cantor's \\\n",
    "    pairing function to return a unique integer value for a pair(x,y) \\ \n",
    "    \"\"\"\n",
    "    comb_arr = 0.5 * (pop_arr + anc_arr) * (pop_arr + anc_arr + 1) + anc_arr \n",
    "    \n",
    "    \"\"\"\n",
    "    Write the combine array to the dasymetric raster using the GeoTransform \\\n",
    "    from the ancillary raster \\\n",
    "    \"\"\"\n",
    "    dasyRast = rast_driver.Create(dasyRaster, cols, rows, 1, \n",
    "                            gdal.GDT_Float64, options=['COMPRESS=LZW'])\n",
    "    dasyRast.SetGeoTransform((ulx, ancRaster.GetGeoTransform()[1], 0, \n",
    "                              uly, 0, ancRaster.GetGeoTransform()[5]))\n",
    "    dasyRast.SetProjection(anc_proj)\n",
    "    dasyRast_b1 =dasyRast.GetRasterBand(1)\n",
    "    dasyRast_b1.WriteArray(comb_arr)\n",
    "    \n",
    "    \"\"\"\n",
    "    Set the NoData value for the dasymetric raster band by running the same \\\n",
    "    Cantor's pairing function on the NoData values from the population raster \\\n",
    "    and ancillary raster. \\\n",
    "    \"\"\"\n",
    "    dasy_nd = 0.5 * (pop_nd + anc_nd) * (pop_nd + anc_nd + 1) + anc_nd\n",
    "    dasyRast_b1.SetNoDataValue(dasy_nd)\n",
    "    dasyRast = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Make the population DataFrame and the dasymetric DataFrame: collect the \\\n",
    "    unique values and counts, rearrange the array via transpose, convert to \\\n",
    "    DataFrame, and rename columns. \\\n",
    "    \"\"\"\n",
    "    dasy_ar_un = np.array(np.unique(comb_arr, return_counts = True)).T\n",
    "    dasy_df = pd.DataFrame(dasy_ar_un, columns = (\"Value\", \"Count\"))\n",
    "    \n",
    "    pop_ar_un = np.array(np.unique(pop_arr, return_counts = True)).T\n",
    "    pop_df = pd.DataFrame(pop_ar_un, columns = (\"Value\", \"Count\"))\n",
    "    \n",
    "    '''\n",
    "    Inverse of Cantor's pairing to get the polygon ID and ancillary class \\\n",
    "    associated with each dasy unit. \\\n",
    "    w = (sqrt(8 * dasy_df['Value'] + 1) - 1) // 2\n",
    "    '''\n",
    "    num = 8*dasy_df['Value'] + 1\n",
    "    dasy_df['w'] = (num.pow(1./2) - 1) // 2\n",
    "    dasy_df['t'] = (dasy_df['w'].pow(2) + dasy_df['w']) / 2\n",
    "    dasy_df['ancID'] = dasy_df['Value'] - dasy_df['t'] \n",
    "    dasy_df['polyID'] = dasy_df['w'] - dasy_df['ancID']\n",
    "\n",
    "    #get rid of unnecessary columns and NoData values\n",
    "    dasy_df = dasy_df[dasy_df['Value'] != dasy_nd].drop(columns = ['w','t'])\n",
    "    pop_df = pop_df[pop_df['Value'] != pop_nd]\n",
    "\n",
    "    #Set variables for DataFrame columns\n",
    "    popIDField = 'polyID'\n",
    "    ancCatName = 'ancID'\n",
    "    dasyAreaField = 'Count'\n",
    "\n",
    "    #Make lists to use later\n",
    "    #All ancillary categories in study area\n",
    "    inAncCatList = list(np.unique(dasy_df[ancCatName]).astype(int))\n",
    "    \n",
    "    '''            \n",
    "    This list will be populated with ancillary categories that are not sampled \\\n",
    "    and do not have preset class densities. \\\n",
    "    '''\n",
    "    unSampledList = []\n",
    "                \n",
    "    #Preset class densities from config.json file\n",
    "    presetData = json.load(open(presetTable))\n",
    "    \n",
    "    '''\n",
    "    Uninhabited classes: ancially classes where people do not live. Classes with \\\n",
    "    a preset class density of 0 \\\n",
    "    '''\n",
    "    unInhabList = [int(presetCat) for presetCat,presetVal in presetData.items()\n",
    "                    if float(presetVal) == 0]\n",
    "    \n",
    "    #Ancillary classes where people can live\n",
    "    InhabList = [cat for cat in inAncCatList if cat not in unInhabList]\n",
    "    \n",
    "    '''\n",
    "    Join the census population counts to the dasymetric DataFrame and calculate \\\n",
    "    population density for the polygon. \\\n",
    "    '''\n",
    "    print (\"Calculating populated area...\")\n",
    "    #Read the source population shapefile with the population count field.\n",
    "    popfeat_df = gp.read_file(popFeat_path)\n",
    "\n",
    "    '''\n",
    "    Set the polygon ID field provided by the user as an index for the \\\n",
    "    population features DataFrame and the population DataFrame for joining and \\\n",
    "    transfering the population count field. \\\n",
    "    '''\n",
    "    popfeat_df.index = popfeat_df[popKeyField]\n",
    "    pop_df.index = pop_df[\"Value\"]\n",
    "    \n",
    "    '''\n",
    "    Join population counts from popfeat_df to the dasymetric DataFrame and the \\\n",
    "    population DataFrame. Rename the field to \"POP_COUNT\" in the dasymetric \\\n",
    "    DataFrame. \\\n",
    "    '''\n",
    "    dasy_df = dasy_df.join(popfeat_df[popCountField], \n",
    "                           on = popIDField).rename(\n",
    "                                   columns = {popCountField: \"POP_COUNT\"}\n",
    "                                   )\n",
    "    pop_df = pop_df.join(popfeat_df[popCountField])\n",
    "\n",
    "    '''\n",
    "    Group the dasymetric units that are associated with inhabitable classes by \\\n",
    "    the census polygon ID and take the sum of the dasymetric area in each \\\n",
    "    group. Rename the column as \"POP_AREA\". \\\n",
    "    POP_AREA = sum(pixels) for inhabitable classes \\\n",
    "    '''\n",
    "    popAreaSum = dasy_df[\n",
    "            dasy_df[ancCatName].isin(InhabList)\n",
    "            ].groupby(popIDField)[dasyAreaField].sum().rename(\"POP_AREA\")\n",
    "\n",
    "    '''\n",
    "    Transfer \"POP_AREA\" from popAreaSum to the dasymetric DataFrame and the \\\n",
    "    population DataFrame. \\\n",
    "    '''      \n",
    "    dasy_df[\"POP_AREA\"] = dasy_df.join(popAreaSum, on = popIDField)[\"POP_AREA\"]\n",
    "    pop_df = pop_df.join(popAreaSum).fillna(0)\n",
    "\n",
    "    '''\n",
    "    Calculate population density for census polygons where populated area is \\\n",
    "    greater than 0. \\\n",
    "    '''\n",
    "    print (\"Calculating population density...\")           \n",
    "    pop_densMask = pop_df[\"POP_AREA\"] > 0\n",
    "    pop_df.loc[pop_densMask, \"POP_DENS\"] = pop_df.loc[\n",
    "            pop_densMask, popCountField] / pop_df.loc[pop_densMask, \"POP_AREA\"]\n",
    "    #replace NaN with 0\n",
    "    pop_df = pop_df.fillna(0)\n",
    "    \n",
    "    '''\n",
    "    Calculate representative population density for ancillary classes that have \\\n",
    "    enough representative samples in the study area. \\\n",
    "    '''\n",
    "    print (\"Selecting representative units...\")\n",
    "    #Create column for the ancillary class that a polygon is representative of \n",
    "    pop_df[\"REP_CAT\"] = 0\n",
    "\n",
    "    '''\n",
    "    For each inhabitable ancillary class, collect polygon IDs of census \\\n",
    "    polygons that meet the user-define criteria for being representative of an \\\n",
    "    ancillary class. \\\n",
    "    '''        \n",
    "    for inAncCat in InhabList:\n",
    "        repUnits_mask = (\n",
    "                dasy_df[\"POP_AREA\"] > float(popAreaMin)\n",
    "                ) & (\n",
    "                        dasy_df[ancCatName] == inAncCat\n",
    "                        )\n",
    "        repUnits = dasy_df.loc[\n",
    "                repUnits_mask, [\n",
    "                        dasyAreaField, popIDField, ancCatName, \"POP_AREA\"\n",
    "                        ]\n",
    "                ]\n",
    "        repUnits[\"PERCENT\"] = repUnits[dasyAreaField] / repUnits[\"POP_AREA\"]                \n",
    "        repUnits = list(\n",
    "                repUnits[repUnits[\"PERCENT\"] >= float(percent)][popIDField]\n",
    "                )\n",
    "                \n",
    "        if len(repUnits) >= float(sampleMin):\n",
    "            pop_df.loc[pop_df['Value'].isin(repUnits), \"REP_CAT\"] = inAncCat\n",
    "            print (\"Class \" \n",
    "                   + str(inAncCat) \n",
    "                   + \" was sufficiently sampled with \" \n",
    "                   + str(len(repUnits)) \n",
    "                   + \" representative source units.\")\n",
    "            \n",
    "            '''\n",
    "            #If ancillary category has no representative polygons and it does \\\n",
    "            not have a preset class density, then add it to the list of \\\n",
    "            unsampled classes. \\\n",
    "            '''\n",
    "        elif str(inAncCat) not in list(presetData):\n",
    "            unSampledList.append(int(inAncCat))\n",
    "            print (\"Class \" \n",
    "                   + str(inAncCat) \n",
    "                   + \" was not sufficiently sampled with only \" \n",
    "                   + str(len(repUnits)) \n",
    "                   + \" representative source units.\")\n",
    "            \n",
    "    #Calculate statistics and make sampling summary table\n",
    "    print (\n",
    "            \"Calculating representative population density for selected\" \n",
    "            \" classes...\"\n",
    "            )\n",
    "    \n",
    "    '''\n",
    "    Create a mask for rows in the dasymetric DataFrame where REP_CAT =! 0. \\\n",
    "    We only want to create summaries for these dasymetric rows because they are \\\n",
    "    associated with representative polygons. \\\n",
    "    '''\n",
    "    rep_mask = pop_df[\"REP_CAT\"] != 0\n",
    "\n",
    "    '''\n",
    "    Calculate sum of census population counts and sum of populated area for \\\n",
    "    each sampled ancillary class. \\\n",
    "    '''\n",
    "    classDens_df = pop_df[rep_mask].groupby(\"REP_CAT\")[\n",
    "            [popCountField, 'POP_AREA']\n",
    "            ].sum().rename(\n",
    "            columns = {popCountField: \"SUM_\" + popCountField, \n",
    "                       \"POP_AREA\": \"SUM_POP_AREA\"}\n",
    "            )\n",
    "            \n",
    "    #Calculate sample density for sampled classes\n",
    "    classDens_df[\"SAMPLEDENS\"] = classDens_df[\n",
    "            \"SUM_\" + popCountField\n",
    "            ] / classDens_df[\"SUM_POP_AREA\"]\n",
    "    classDens_df[\"METHOD\"] = \"Sampled\"\n",
    "    classDens_df[\"CLASSDENS\"] = classDens_df[\"SAMPLEDENS\"]\n",
    "                    \n",
    "    #Add preset densities to summary table\n",
    "    if presetTable:\n",
    "        print (\"Adding preset values to the summary table...\")\n",
    "        for preset_cat in list(presetData):\n",
    "            classDens_df.loc[int(preset_cat), \"CLASSDENS\"] = presetData[\n",
    "                    preset_cat\n",
    "                    ]\n",
    "            classDens_df.loc[int(preset_cat), \"METHOD\"] = 'Preset'\n",
    "            \n",
    "    # For all sampled and preset classes, calculate a population estimate.\n",
    "    print (\n",
    "            \"Calculating population estimate for sampled and preset classes...\"\n",
    "            )            \n",
    "    #Get representative population densities from class density DataFrame.\n",
    "    dasy_df = dasy_df.join(classDens_df['CLASSDENS'], on = ancCatName).fillna(0)\n",
    "    \n",
    "    '''\n",
    "    #Set mask for dasy_df that will limit ancillary categories to those in the \\\n",
    "    class density DataFrame. \\\n",
    "    '''\n",
    "    popEst_mask = dasy_df[ancCatName].isin(classDens_df.index)\n",
    "    \n",
    "    '''\n",
    "    POP_EST = area of the dasymetric unit \\\n",
    "    * the representative population density of the ancillary class associated \\\n",
    "    with the dasymetric unit \\\n",
    "    '''\n",
    "    dasy_df[\"POP_EST\"] = 0\n",
    "    dasy_df.loc[popEst_mask, \"POP_EST\"] = dasy_df.loc[\n",
    "            popEst_mask, dasyAreaField\n",
    "            ] * dasy_df.loc[\n",
    "                    popEst_mask, 'CLASSDENS'\n",
    "                    ]\n",
    "    \n",
    "    # Intelligent areal weighting for unsampled classes            \n",
    "    print (\"Performing intelligent areal weighting for unsampled classes...\")\n",
    "    if unSampledList:\n",
    "        '''\n",
    "        Calculate representative population densities for unsampled ancillary \\\n",
    "        classes using IAW \\\n",
    "        '''\n",
    "        unsampled_mask = dasy_df[ancCatName].isin(unSampledList)\n",
    "        \n",
    "        '''\n",
    "        Populate remainining area of each dasymetric unit as the area of \\\n",
    "        dasymetric units associated with unsampled classes and 0 everywhere \\\n",
    "        else. \\\n",
    "        '''\n",
    "        dasy_df[\"REM_AREA\"] = 0\n",
    "        dasy_df.loc[unsampled_mask, \"REM_AREA\"] = dasy_df.loc[\n",
    "                unsampled_mask, dasyAreaField\n",
    "                ]\n",
    "        \n",
    "        '''                          \n",
    "        For each polygon, sum the remaining area and sum the population that \\\n",
    "        has already been estimated for sampled/preset classes. \\\n",
    "        '''\n",
    "        popEstSum = dasy_df.groupby(popIDField)[\n",
    "                [\"POP_EST\", \"REM_AREA\"]\n",
    "                ].sum()\n",
    "        \n",
    "        '''\n",
    "        Join popEstSum to dasy_df to transfer the sum of population estimates \\\n",
    "        and the sum of remaining area to the dasymetric DataFrame. \\\n",
    "        '''\n",
    "        dasy_df = dasy_df.join(popEstSum[\"POP_EST\"], on = popIDField, \n",
    "                               rsuffix = \"poly\")\n",
    "        dasy_df = dasy_df.join(popEstSum[\"REM_AREA\"], on = popIDField, \n",
    "                               rsuffix = \"poly\")\n",
    "        \n",
    "        '''\n",
    "        Calculate a population difference between the census population and the \\\n",
    "        population estimated for sampled/preset ancillary classes. \\\n",
    "        '''\n",
    "        dasy_df[\"POP_DIFF\"] = dasy_df[\"POP_COUNT\"] - dasy_df[\"POP_ESTpoly\"]\n",
    "        \n",
    "        '''\n",
    "        Calculate an initial population estimate for dasymetric units \\\n",
    "        associated with unsampled ancillary classes and polygons where the \\\n",
    "        sampled/preset population estimates did not exceed the census \\\n",
    "        population count. \\\n",
    "        '''\n",
    "        diff_mask = (dasy_df[ancCatName].isin(unSampledList) &\n",
    "                     dasy_df['REM_AREApoly'] !=0 )\n",
    "        dasy_df.loc[diff_mask, \"POP_EST\"] = (\n",
    "                dasy_df.loc[diff_mask, \"POP_DIFF\"].clip(0) * \n",
    "                dasy_df.loc[diff_mask, \"REM_AREA\"] / \n",
    "                dasy_df.loc[diff_mask, \"REM_AREApoly\"])\n",
    "        '''\n",
    "        Sum total initial population estimates and remaining area for \\\n",
    "        dasymetric units used to calculate initial population estimates for \\\n",
    "        unsampled ancillary classes. \\\n",
    "        '''\n",
    "        ancCat_sum = dasy_df[diff_mask].groupby(ancCatName)[\n",
    "                [\"POP_EST\" , \"REM_AREA\"]\n",
    "                ].sum()\n",
    "        \n",
    "        '''\n",
    "        Calculate the representative population density for unsampled classes \\\n",
    "        using ancCat_sum and update the class density DataFrame. \\\n",
    "        '''\n",
    "        for cat in ancCat_sum.index:\n",
    "            classDens_df.loc[cat, \"CLASSDENS\"] = ancCat_sum.loc[cat, \n",
    "                            \"POP_EST\"] / ancCat_sum.loc[cat, \n",
    "                                           \"REM_AREA\"]\n",
    "            classDens_df.loc[cat, \"METHOD\"] = \"IAW\"   \n",
    "        \n",
    "        '''\n",
    "        Add representative population densities for unsampled classes in the \\\n",
    "        dasymetric DataFrame. \\\n",
    "        '''\n",
    "        dasy_df.loc[unsampled_mask, 'CLASSDENS'] = dasy_df.loc[\n",
    "                unsampled_mask\n",
    "                ].join(classDens_df.loc[\n",
    "                        ancCat_sum.index, 'CLASSDENS'\n",
    "                        ], \n",
    "                on = ancCatName, rsuffix = \"_classDens\")['CLASSDENS_classDens']\n",
    "        \n",
    "        '''\n",
    "        Calculate new population estimates using representative population \\\n",
    "        densities for unsampled classes. \\\n",
    "        POP_EST = dasymetric area * class density \\\n",
    "        '''\n",
    "        dasy_df.loc[unsampled_mask, \"POP_EST\"] = dasy_df.loc[unsampled_mask, \n",
    "                   dasyAreaField] * dasy_df.loc[unsampled_mask, \n",
    "                               'CLASSDENS']\n",
    "                               \n",
    "        # End of intelligent areal weighting\n",
    "             \n",
    "    # Perform final calculations to ensure pycnophylactic integrity\n",
    "    print (\n",
    "            \"Performing final calculations to ensure pycnophylactic\" \\\n",
    "            \" integrity...\"\n",
    "            )\n",
    "    '''\n",
    "    For each dasymetric unit, use the ratio of the estimated population to the \\\n",
    "    total population estimated for the polygon associated with the dasymetric \\\n",
    "    unit to redistribute the census population.  \\\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    if the sum of population densities within the source unit is equal to 0 \\\n",
    "    set the POP_EST for those to 1 (i.e., area weighting (equation 5)) \\\n",
    "    '''\n",
    "\n",
    "    idx = (dasy_df\n",
    "            .groupby(popIDField)\n",
    "            .filter(\n",
    "                lambda s: s['POP_EST'].sum() == 0 and\n",
    "                          s['POP_COUNT'].sum() > 0\n",
    "                    ).index\n",
    "            )\n",
    "\n",
    "    dasy_df.loc[idx, 'POP_EST'] = 1\n",
    "    \n",
    "    #Sum population estimates by polygon.\n",
    "    popEstsum = dasy_df.groupby(popIDField)[\"POP_EST\"].sum()\n",
    "    \n",
    "    dasy_df[\"TOTALFRACT\"] = dasy_df[\"POP_EST\"] / dasy_df.join(popEstsum, \n",
    "           on = popIDField, rsuffix = \"SUM\")[\"POP_ESTSUM\"]\n",
    "    dasy_df[\"NEW_POP\"] = dasy_df[\"TOTALFRACT\"] * dasy_df[\"POP_COUNT\"]\n",
    "    dasy_df[\"NEWDENSITY\"] = dasy_df[\"NEW_POP\"] / dasy_df[dasyAreaField]\n",
    "    \n",
    "    #Replace nan with 0\n",
    "    dasy_df = dasy_df.fillna(0)\n",
    "    \n",
    "    #export dasy table to .csv\n",
    "    dasy_df.to_csv(dasyWorkTable, header = True)\n",
    "               \n",
    "    #export pop_df to .csv\n",
    "    pop_df.fillna(0).to_csv(popWorkTable, header = True)\n",
    "            \n",
    "    #export classDens_df to sampling summary table\n",
    "    classDens_df.to_csv(os.path.join(out_dir, \"SamplingSummaryTable.csv\"), \n",
    "                        header = True)\n",
    "    \n",
    "    #Create final population density raster.\n",
    "    print (\"Creating population density raster...\")\n",
    "    #Create population density array.\n",
    "    dasy_lut = dasy_df[['Value', 'NEWDENSITY']].set_index('Value')\n",
    "    dasy_lut.loc[dasy_nd, 'NEWDENSITY'] = -999 #NoData value from comb_arr\n",
    "    dens_df = pd.DataFrame(np.ravel(comb_arr)).join(dasy_lut, on = 0)\n",
    "    dens_ar = np.array(\n",
    "            dens_df['NEWDENSITY']\n",
    "            ).reshape(\n",
    "                    (comb_arr.shape[0], comb_arr.shape[1])\n",
    "                    )\n",
    "    \n",
    "    #Write array to population density raster.\n",
    "    densRast = rast_driver.Create(densityRaster, cols, rows, 1, \n",
    "                                  gdal.GDT_Float32, options=['COMPRESS=LZW'])\n",
    "    densRast.SetGeoTransform((ulx, ancRaster.GetGeoTransform()[1], 0, \n",
    "                              uly, 0, ancRaster.GetGeoTransform()[5]))\n",
    "    densRast.SetProjection(anc_proj)\n",
    "    densRast_b1 =densRast.GetRasterBand(1)\n",
    "    densRast_b1.WriteArray(dens_ar)\n",
    "    densRast_b1.SetNoDataValue(-999)\n",
    "    densRast = None\n",
    "    \n",
    "    print (\"All outputs from this tool can be found in \" + out_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47015aa-985e-4b11-8f8d-8d3db05c7144",
   "metadata": {},
   "source": [
    "## Dasymetric Implementation \n",
    "\n",
    "### EPA Demo Data\n",
    "\n",
    "First, we will test with the EPA's provided sample data. Then we will use our own data for the AIST Hampton Roads study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40dc52e-dec3-4235-aba1-aa3cb99e1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EPA data\n",
    "popFeat_path = \"./data/2010_blocks_DE.shp\"\n",
    "popCountField = 'POP10'\n",
    "popKeyField = 'polyID'\n",
    "ancRaster_path = './data/nlcd_2011_DE.tif'\n",
    "out_dir = './output'\n",
    "uninhab_path = './data/uninhab_DE.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31919149-a4e6-4a2e-80ea-511e39c26688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population_features path: ./data/2010_blocks_DE.shp\n",
      "population_count_field: POP10\n",
      "population_key_field: polyID\n",
      "ancillary_raster: ./data/nlcd_2011_DE.tif\n",
      "uninhabited_file: ./data/uninhab_DE.shp\n",
      "The minimum populated area of a representative unit is 1\n",
      "The minimum sample size is 5\n",
      "The percent is 0.95\n",
      "The NoData value for the population raster is 0\n",
      "The NoData value for the ancillary raster is 0\n",
      "Creating population raster...\n",
      "Creating dasymetric units...\n",
      "Calculating populated area...\n",
      "Calculating population density...\n",
      "Selecting representative units...\n",
      "Class 21 was sufficiently sampled with 172 representative source units.\n",
      "Class 22 was sufficiently sampled with 455 representative source units.\n",
      "Class 23 was sufficiently sampled with 581 representative source units.\n",
      "Class 24 was sufficiently sampled with 304 representative source units.\n",
      "Class 31 was sufficiently sampled with 26 representative source units.\n",
      "Class 41 was sufficiently sampled with 12 representative source units.\n",
      "Class 42 was not sufficiently sampled with only 3 representative source units.\n",
      "Class 43 was sufficiently sampled with 9 representative source units.\n",
      "Class 52 was not sufficiently sampled with only 2 representative source units.\n",
      "Class 71 was sufficiently sampled with 6 representative source units.\n",
      "Class 81 was not sufficiently sampled with only 2 representative source units.\n",
      "Class 82 was sufficiently sampled with 284 representative source units.\n",
      "Class 90 was sufficiently sampled with 106 representative source units.\n",
      "Calculating representative population density for selected classes...\n",
      "Adding preset values to the summary table...\n",
      "Calculating population estimate for sampled and preset classes...\n",
      "Performing intelligent areal weighting for unsampled classes...\n",
      "Performing final calculations to ensure pycnophylactic integrity...\n",
      "Creating population density raster...\n",
      "All outputs from this tool can be found in ./output\n"
     ]
    }
   ],
   "source": [
    "#Run the tool\n",
    "dasy_map(popFeat_path = popFeat_path, #path to population polygons\n",
    "         popCountField = popCountField, #field name for population count\n",
    "         popKeyField = popKeyField, #key ID \n",
    "         ancRaster_path = ancRaster_path, #path to landcover raster \n",
    "         out_dir = out_dir, #path to output folder  \n",
    "         popAreaMin = 1, \n",
    "         sampleMin = 5, \n",
    "         percent = 0.95, \n",
    "         uninhab_path = uninhab_path, \n",
    "         anc_nd = 0, \n",
    "         pop_nd = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e93c3b-2826-47b0-9dac-95bf4f74613b",
   "metadata": {},
   "source": [
    "Success! Now we will try with our own data after accounting for unhabited areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb8126-dca8-427c-b53b-9be9b3a3d19d",
   "metadata": {},
   "source": [
    "### AIST Hampton Roads Data Modification\n",
    "In this section, we will go through how to modify the data on the fly in preparation for dasymetric mapping. \n",
    "Here are the steps:\n",
    "- Create an empty feature class for uninhabited areas mask\n",
    "- Filter for census blocks with 0 people and add those blocks to the mask\n",
    "- Take road data, make into polygons, and add to the mask\n",
    "- Output census block data from API to a shape file\n",
    "- Output mask to a shape file\n",
    "- Output land cover as GeoTiff \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c099cba-9988-4927-8133-508db3f9b789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7cebf-4efb-45e4-b26b-fbb5e71079a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1add747",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "\n",
    "<b>License:</b> The code in this notebook is licensed under the Apache License, Version 2.0. Digital Earth Australia data is licensed under the Creative Commons by Attribution 4.0 license.\n",
    "\n",
    "<b>Contact:</b> If you need assistance, please post a question on the Open Data Cube Slack channel or on the GIS Stack Exchange using the open-data-cube tag (you can view previously asked questions here). If you would like to report an issue with this notebook, you can file one on Github.\n",
    "\n",
    "<b>Last modified:</b> May 2020\n",
    "\n",
    "<b>Compatible datacube version:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4c2e9",
   "metadata": {},
   "source": [
    "## Tags \n",
    "Browse all available tags on the VA Data Cube User Guide's Tags Index "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
